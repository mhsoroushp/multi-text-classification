{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.offline import iplot\n",
    "from tqdm import tqdm\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '../bbc-text.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(root_path)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['encoded_text'] = df['category'].astype('category').cat.codes\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit\"\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "    \n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "# if no improvment in 10 epoch then restore the best weights\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification, AutoTokenizer \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "#model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=5)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-multilingual-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_texts = df['text'].to_list()\n",
    "data_labels = df['encoded_text'].to_list()\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(data_texts, data_labels, test_size = 0.2, random_state = 0 )\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(train_texts, train_labels, test_size = 0.01, random_state = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input pipelines\n",
    "max_length = 128\n",
    "batch_size = 32\n",
    "\n",
    "train_encodings = tokenizer(list(train_texts), truncation=True, padding=True, max_length=max_length)\n",
    "val_encodings = tokenizer(list(val_texts), truncation=True, padding=True, max_length=max_length)\n",
    "test_encodings = tokenizer(list(test_texts), truncation=True, padding=True, max_length=max_length)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels)).batch(batch_size)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), val_labels)).batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), test_labels)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, validation_data=val_dataset, epochs=10, callbacks=[tensorboard_callback, early_stopping])\n",
    "\n",
    "model.save_pretrained(\"./results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plotting Loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Loss Plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "# Accuracy Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['sparse_categorical_accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"./results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "print(\"Test Loss\", test_loss)\n",
    "print(\"Test Accuracy\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer_fine_tuned = AutoTokenizer.from_pretrained(\"./results\")\n",
    "model_fine_tuned = TFDistilBertForSequenceClassification.from_pretrained(\"./results\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = test_texts[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer_fine_tuned(test_text, truncation=True, padding=True, max_length=max_length, return_tensors='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model_fine_tuned(**encoding).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class = tf.argmax(logits, axis=-1).numpy()[0]\n",
    "print(\"predicted_class:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels[5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".emailvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
